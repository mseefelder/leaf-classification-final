{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf Classification - Relatório 2\n",
    "\n",
    "Modelos lineares/simples:\n",
    "\n",
    "* Bayesiano (sklearn.naive_bayes)\n",
    "\n",
    "Modelo não-linear:\n",
    "\n",
    "* KNeighborsClassifier\n",
    "\n",
    "Árvore:\n",
    "\n",
    "* Decision tree\n",
    "\n",
    "Ensemble:\n",
    "\n",
    "* Random Forests\n",
    "\n",
    "Redes neurais:\n",
    "\n",
    "* Multi-layer Perceptron ([sklearn.neural_network.MLPClassifier](http://scikit-learn.org/stable/modules/neural_networks_supervised.html#neural-networks-supervised))\n",
    "\n",
    "SVM:\n",
    "\n",
    "* SVC ([sklearn.svm](http://scikit-learn.org/stable/modules/svm.html#svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratando os dados\n",
    "\n",
    "As próximas duas céclulas preparam os dados pra rodar os classificadores. No final:\n",
    "\n",
    "* `train` tem o conjunto de treinamento\n",
    "* `labels` tem as respostas pra todas as amostras do conjunto de treinamento\n",
    "* `test` e `test_ids` pode ignorar, seria importante só se a gente fosse fazer submit no kaggle\n",
    "* `classes` tem a lista de classes existentes no nosso problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def warn(*args, **kwargs): pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Swiss army knife function to organize the data\n",
    "\n",
    "def encode(train, test):\n",
    "    le = LabelEncoder().fit(train.species) \n",
    "    labels = le.transform(train.species)           # encode species strings\n",
    "    classes = list(le.classes_)                    # save column names for submission\n",
    "    test_ids = test.id                             # save test ids for submission\n",
    "    \n",
    "    train = train.drop(['species', 'id'], axis=1)  \n",
    "    test = test.drop(['id'], axis=1)\n",
    "    \n",
    "    return train, labels, test, test_ids, classes\n",
    "\n",
    "train, labels, test, test_ids, classes = encode(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Começam os testes\n",
    "\n",
    "Importamos um bando de coisas: (métricas de avaliação)[http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics] (primeira linha de import), métodos de classificação (linhas 2 a 7 de import) e  umas utilidades pra fazer cross-validation(última linha de import).\n",
    "\n",
    "Aí colocamos uma pá de classificadores num array e vamos testá-los. Ali embaixo defini uma função `crossValidate` pra fazer a cross-validation e printar as coisas bonitinho, **mas antes disso** criei aquele `folds`.\n",
    "\n",
    "`folds` é a o que a cross-validation usa pra dividir o nosso conjunto sempre nos mesmos 10 subgrupos e fazer a mesma cross-validation. Não precisa se preocupar com isso.\n",
    "\n",
    "## Sobre as métricas:\n",
    "\n",
    "É uma boa revermos se essas métricas fazem sentido mesmo. Do jeito que elas estão sendo feitas ele calcula a métrica (precision ou recall respectivamente) pra cada classe e faz uma média de todos os valores achados pois, como pode se ver dentro da função `crossValidate`, elas estão sendo feitas com o parâmetro `average='macro'`. Existem outros parâmetros que podemos passar pra `average`, mas não sei se faz sentido. Mostrar *precision* e *average* pra cada classe com cada classificador ia ficar muita coisa e não ia ser nada esclarecedor...\n",
    "\n",
    "Do jeito que está `recall_score` funciona igual funcionaria `accuracy_score`.\n",
    "\n",
    "No notebook do Kaggle, o cara usa uma métrica (`log_loss`)[http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss], mas não cheguei a ler sobre ele e por que ela seria útil. Vale ressaltar que ela não usa como entrada o resultado default de `cross_val_predict`, por isso tive que fazer:\n",
    "\n",
    "```\n",
    "train_predictions_proba = cross_val_predict(clf, train, labels, n_jobs=-1, cv=folds, method='predict_proba')\n",
    "print(\"LogLoss: \", log_loss(labels, train_predictions_proba))\n",
    "```\n",
    "\n",
    "*(comentei porque dava problema de overflow)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "KNeighborsClassifier\n",
      "****Results****\n",
      "Precision:  0.898580375853\n",
      "Recall:  0.891919191919\n",
      "==============================\n",
      "SVC\n",
      "****Results****\n",
      "Precision:  0.815300943805\n",
      "Recall:  0.80101010101\n",
      "==============================\n",
      "DecisionTreeClassifier\n",
      "****Results****\n",
      "Precision:  0.697049471292\n",
      "Recall:  0.687878787879\n",
      "==============================\n",
      "RandomForestClassifier\n",
      "****Results****\n",
      "Precision:  0.90722052919\n",
      "Recall:  0.89797979798\n",
      "==============================\n",
      "GaussianNB\n",
      "****Results****\n",
      "Precision:  0.750538610202\n",
      "Recall:  0.611111111111\n",
      "==============================\n",
      "LinearDiscriminantAnalysis\n",
      "****Results****\n",
      "Precision:  0.980466671376\n",
      "Recall:  0.977777777778\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss, recall_score, precision_recall_fscore_support, precision_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis()\n",
    "]\n",
    "\n",
    "# Define a KFold for cross-validation\n",
    "# Guarantees all cross validations are made with the same split\n",
    "folds = StratifiedKFold(n_splits=10, random_state=None, shuffle=False)\n",
    "\n",
    "# Cross validation\n",
    "def crossValidate(classifiers, train, labels, folds):\n",
    "    for clf in classifiers:\n",
    "        name = clf.__class__.__name__\n",
    "    \n",
    "        print(\"=\"*30)\n",
    "        print(name)\n",
    "    \n",
    "        print('****Results****')\n",
    "        train_predictions = cross_val_predict(clf, train, labels, n_jobs=-1, cv=folds)#clf.predict(X_test)\n",
    "        print(\"Precision: \", precision_score(labels, train_predictions, average='macro'))\n",
    "        print(\"Recall: \", recall_score(labels, train_predictions, average='macro'))\n",
    "        \n",
    "        #train_predictions_proba = cross_val_predict(clf, train, labels, n_jobs=-1, cv=folds, method='predict_proba')\n",
    "        #print(\"LogLoss: \", log_loss(labels, train_predictions_proba))\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    \n",
    "# Test for classifiers above\n",
    "crossValidate(classifiers, train, labels, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes com redes neurais\n",
    "\n",
    "A partir daqui testo com redes neurais (MultiLayer Perceptron)[http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.predict].\n",
    "\n",
    "Primeiro pra 1 camada com os tamanhos 1, 99 e 198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "MLPClassifier\n",
      "****Results****\n",
      "Precision:  0.000877698763534\n",
      "Recall:  0.0111111111111\n",
      "==============================\n",
      "MLPClassifier\n",
      "****Results****\n",
      "Precision:  0.911367448489\n",
      "Recall:  0.9\n",
      "==============================\n",
      "MLPClassifier\n",
      "****Results****\n",
      "Precision:  0.936460649339\n",
      "Recall:  0.929292929293\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "MLPs = [\n",
    "    MLPClassifier(hidden_layer_sizes=(1,)),\n",
    "    MLPClassifier(hidden_layer_sizes=(99,)),\n",
    "    MLPClassifier(hidden_layer_sizes=(198,))\n",
    "]\n",
    "\n",
    "crossValidate(MLPs, train, labels, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padronizando os dados\n",
    "\n",
    "Aí eu li que padronizando seria melhor pq o MLP é sensível à escala. Usei esse StandardScaler do sklearn que padroniza automático e rodei de novo pra doi métodos lá de cima (que nos meus testes foram os únicos que se mostraram sensíveis à escala) e pras redes neurais de novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "KNeighborsClassifier\n",
      "****Results****\n",
      "Precision:  0.976946622401\n",
      "Recall:  0.973737373737\n",
      "==============================\n",
      "SVC\n",
      "****Results****\n",
      "Precision:  0.98034423489\n",
      "Recall:  0.977777777778\n",
      "==============================\n",
      "MLPClassifier\n",
      "****Results****\n",
      "Precision:  0.00517677797802\n",
      "Recall:  0.0242424242424\n",
      "==============================\n",
      "MLPClassifier\n",
      "****Results****\n",
      "Precision:  0.98406285073\n",
      "Recall:  0.982828282828\n",
      "==============================\n",
      "MLPClassifier\n",
      "****Results****\n",
      "Precision:  0.989868380777\n",
      "Recall:  0.988888888889\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train)\n",
    "\n",
    "crossValidate(classifiers[:2]+MLPs, scaler.transform(train), labels, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ainda temos que:\n",
    "\n",
    "* Fazer a matriz de distâncias ordenando por classe (faltou do relatorio 1)\n",
    "* definir quais metricas de distância vamos user e porque\n",
    "* fazer uns gráficos pra os valores das métricas pros classificadores\n",
    "* testar novas topologias de rede neural (mais camadas, distribuir de jeitos diferentes)\n",
    "* se der tempo: usar (algum método de otimização de hiperparâmetros)[http://scikit-learn.org/stable/modules/grid_search.html] pra fazer uma rede neural melhor\n",
    "* ler sobre os classificadores que usamos\n",
    "* avaliar os resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
